{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将输入数据通过 encoder(例如CNN) 进行降维或提取特征，得到 feature map, 然后再通过 decoder 生成输出数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search 是每一步保留概率最大的那个词，而beam search是每一步保留概率最大的几个词(3或5个)，这样的话容错率增加，能更准确的找到概率最大的句子，而不是概率最大的相关词组成的句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在transformer中，利用三角函数计算对应位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量没办法解决一词多义的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMO模型是一个双层双向的LSTM模型，在实际使用中，根据训练得到的权重取值决定词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 模型可以进行并行计算，可以节省时间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于 Transformer 模型，其网络输入往往是不定长的，则 batch normalization 不适于使用，应使用 layer normalizatio代替。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了保留句子中词组的位置信息，所以需要嵌入位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention 这种方式只使用内部信息，key 和 value 以及 query 的值只和输入原文有关，在原文中的每个词跟该句子中的所有词进行Attention计算，相当于寻找原文内部的关系，由 query 和 key 计算出权重后，再计算 value。 \n",
    "多次使用 self-attention 进行取值后，将其融合拼接映射到原维度，就是multi-attrntion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 的 basic unit 是 Transformer 的解码器（decoder）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 NLP 任务中使用 GPT 时分为两个阶段，第一阶段是用 GPT 语言模型进行预训练， 得到词向量； 第二阶段根据实际下游任务（比如分类问题或者文本相似性判断等）进行设计网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 在训练双向语言模型时以一定概率将少量的词替成了 Mask（80%训练时间） 或者另一个随机的词（10%训练时间），然后对其进行前后文预测来进行训练，这种训练方式就是 masked language model in BERT 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 模型的输入包括 Token Embeddings（词向量）， Position Embeddings（可学习的向量），Segment Embeddings（对每个语句嵌入不同的可区分的向量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在面向特定任务时，将 BERT 与一个额外的输出层结合而形成的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 模型是单向的 Transformer 模型， BERT 是双向的 Transformer 模型， GPT2 是在 GPT 模型的基础上迭代更深更多层的模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
